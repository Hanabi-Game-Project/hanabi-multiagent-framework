# defining the session
session.n_parallel = 96
session.n_parallel_eval = 3000
session.epochs = 5000
session.n_train_steps = 4
session.eval_freq = 500

# define schedule for annealing of beta_is
schedule_beta_is.value_start = 0.4
schedule_beta_is.value_end = 1
schedule_beta_is.steps = 3500

# define the agent parameters
agent_0/RlaxRainbowParams.n_network		         = 3
agent_0/RlaxRainbowParams.train_batch_size       = 32
agent_0/RlaxRainbowParams.target_update_period   = 500
agent_0/RlaxRainbowParams.discount               = 0.99
agent_0/RlaxRainbowParams.epsilon                = 0.1
agent_0/RlaxRainbowParams.learning_rate          = 6.25e-5
agent_0/RlaxRainbowParams.layers                 = [512, 512]
agent_0/RlaxRainbowParams.use_double_q           = True
agent_0/RlaxRainbowParams.use_priority           = True
agent_0/RlaxRainbowParams.use_distribution	 	 = True
agent_0/RlaxRainbowParams.use_noisy_network      = True
agent_0/RlaxRainbowParams.experience_buffer_size = 65536
agent_0/RlaxRainbowParams.seed                   = 42
agent_0/RlaxRainbowParams.n_atoms                = 31 # for hanabi small
agent_0/RlaxRainbowParams.atom_vmax              = 15 # for hanabi small
agent_0/RlaxRainbowParams.beta_is                = @schedule_beta_is()
agent_0/RlaxRainbowParams.priority_w			 = 0.6
agent_0/RlaxRainbowParams.history_size           = 1

agent_0/RewardShapingParams.shaper               = False
agent_0/RewardShapingParams.min_play_probability = 0.8
agent_0/RewardShapingParams.w_play_penalty	 	 = 0
agent_0/RewardShapingParams.m_play_penalty		 = 0
agent_0/RewardShapingParams.w_play_reward	 	 = 0
agent_0/RewardShapingParams.m_play_reward		 = 0
agent_0/RewardShapingParams.penalty_last_of_kind = 0
