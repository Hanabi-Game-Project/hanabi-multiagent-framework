# defining the session
session.n_parallel = 32
session.n_parallel_eval = 1000
session.epochs = 3500
session.n_train_steps = 2
session.eval_freq = 500

# define schedule for annealing of beta_is
schedule_beta_is.value_start = 0.4
schedule_beta_is.value_end = 1
schedule_beta_is.steps = 3500

# define the agent parameters
agent_0/RlaxRainbowParams.train_batch_size       = 64
agent_0/RlaxRainbowParams.target_update_period   = 500
agent_0/RlaxRainbowParams.discount               = 0.99
agent_0/RlaxRainbowParams.epsilon                = 0
agent_0/RlaxRainbowParams.learning_rate          = 6.25e-5
agent_0/RlaxRainbowParams.layers                 = [512, 512]
agent_0/RlaxRainbowParams.use_double_q           = True
agent_0/RlaxRainbowParams.use_priority           = True
agent_0/RlaxRainbowParams.use_distribution       = True
agent_0/RlaxRainbowParams.use_noisy_network      = True
agent_0/RlaxRainbowParams.use_boltzmann_exploration = False
agent_0/RlaxRainbowParams.experience_buffer_size = 65536
agent_0/RlaxRainbowParams.seed                   = 42
agent_0/RlaxRainbowParams.n_atoms                = 31 # for hanabi small
agent_0/RlaxRainbowParams.atom_vmax              = 15 # for hanabi small
agent_0/RlaxRainbowParams.beta_is                = @schedule_beta_is()
agent_0/RlaxRainbowParams.priority_w			 = 0.6
agent_0/RlaxRainbowParams.history_size           = 1

# negative value >> penalty
# positiove value >> reward
agent_0/RewardShapingParams.shaper              	 = True
agent_0/RewardShapingParams.min_play_probability 	 = 0.8
agent_0/RewardShapingParams.w_play_penalty	 	 = 0
agent_0/RewardShapingParams.m_play_penalty		 = 0
agent_0/RewardShapingParams.w_play_reward	 	 	 = 0
agent_0/RewardShapingParams.m_play_reward		 	 = 0
agent_0/RewardShapingParams.penalty_last_of_kind   = 0
