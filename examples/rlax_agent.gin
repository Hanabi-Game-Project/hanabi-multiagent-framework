# defining the session
session.n_parallel                               = 1280
session.n_parallel_eval                          = 5120
session.epochs                                   = 25

# define schedule for annealing of beta_is
#schedule_beta_is.value_start                    = 0.4
#schedule_beta_is.value_end                      = 1
#schedule_beta_is.epochs                         = 3500

# define the agent parameters
agent_0/RlaxRainbowParams.train_batch_size       = 32
agent_0/RlaxRainbowParams.target_update_period   = 300
agent_0/RlaxRainbowParams.discount               = 0.99
agent_0/RlaxRainbowParams.epsilon                = 0.0
agent_0/RlaxRainbowParams.learning_rate          = 6.25e-5
agent_0/RlaxRainbowParams.layers                 = [512, 512]
agent_0/RlaxRainbowParams.use_double_q           = True
agent_0/RlaxRainbowParams.use_priority           = True
agent_0/RlaxRainbowParams.experience_buffer_size = 65536
agent_0/RlaxRainbowParams.seed                   = 42
agent_0/RlaxRainbowParams.n_atoms                = 31 # for hanabi small
agent_0/RlaxRainbowParams.atom_vmax              = 15 # for hanabi small
#agent_0/RlaxRainbowParams.beta_is                = @schedule_beta_is()
agent_0/RlaxRainbowParams.priority_w			 = 0.6
agent_0/RlaxRainbowParams.history_size           = 1

agent_0/PBTParams.population_size                = 2
agent_0/PBTParams.discard_percent                = 0.8
agent_0/PBTParams.individual_reward_shaping      = False
agent_0/PBTParams.life_span                      = 1
agent_0/PBTParams.generations                    = 35
agent_0/PBTParams.saver_threshold                = 0.1
agent_0/PBTParams.pool_path                      = 'pool'
agent_0/PBTParams.use_db                         = False
agent_0/PBTParams.db_path                        = 'obs.db'
agent_0/PBTParams.w_diversity                    = 10
agent_0/PBTParams.obs_no                         = 10000
agent_0/PBTParams.n_mean                         = 5

agent_0/RewardShapingParams.use_reward_shaping   = False
agent_0/RewardShapingParams.min_play_probability = 0.7
agent_0/RewardShapingParams.w_play_probability	 = 0.1
agent_0/RewardShapingParams.penalty_last_of_kind = 0.1

agent_0/PBTParams.change_buffersize              = True
agent_0/PBTParams.buffersize_start_factor        = 4
agent_0/PBTParams.buffersize_factor              = 2

agent_0/PBTParams.change_alpha                   = True
agent_0/PBTParams.factor_alpha                   = 1.05
agent_0/PBTParams.alpha_min                      = 0.3
agent_0/PBTParams.alpha_max                      = 1
agent_0/PBTParams.alpha_sample_size              = 14

agent_0/PBTParams.change_learning_rate           = True
agent_0/PBTParams.lr_factor                      = 1.2 # check later in pbt code
agent_0/PBTParams.lr_min                         = 1e-4
agent_0/PBTParams.lr_max                         = 1e-6
agent_0/PBTParams.lr_sample_size                 = 20

agent_0/PBTParams.change_min_play_probability    = True
agent_0/PBTParams.change_w_play_probability      = True
agent_0/PBTParams.min_play_probability_pbt       = 0.05
agent_0/PBTParams.w_play_probability_pbt         = 0.05

agent_0/PBTParams.change_penalty_last_of_kind    = True
agent_0/PBTParams.penalty_last_of_kind_pbt       = 0.1



